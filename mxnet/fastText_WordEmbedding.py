'''
使用fastText训练词向量
用小说平凡的世界作为语料，使用jieba分词，训练的中文词向量效果不好，可能语料太少

用600M大小的enwiki9.txt作为语料，训练的英文词向量效果简单看还可以，下面的测试基本符合预期：
father neighors: son brother grandfather mother
berlin-germany+france = paris
son - father + mother = daughter\wife\daughters
females neighbors: males median
see neighbors: list disambiguations

用1.4G大小的wiki.zh.corpus.txt作为语料，训练的中文向量效果简单看还可以，下面的测试符合预期：
“父亲”的邻居词有“母亲”、“祖父”、“哥哥”等社会关系词
“柏林”之于“德国”相当于巴黎之于法国
儿子之于父亲，相当于女儿之于母亲
米的邻居词有公尺
由于的邻居词有因此、因为


btw, How can I load .vec file (pretrained or generated by Fasttext or GloVe/other softwares) to Fasttext to actually use it? Is it possible?
A: it is impossible! You can not recovery a fastText model from .vec files, either.
'''
# coding: UTF-8
import fasttext
import mxnet.gluon as gluon
import jieba
import torch
import numpy as np
import random
import torch.utils.data.dataset as dataset
import torch.utils.data.dataloader as dataloader
import torch.nn as nn
import torch.nn.functional as F
import torch.jit
from io import open
import glob
import os
import math
import hashlib
import pickle
import re


def loadData(filename, isChn):
    example_list = list()
    with open(filename, "r", encoding='utf8') as f:
        for line in f:
            line = line[:-1]
            if isChn:
                words = list(jieba.cut(line))
                example_list.append(" ".join(words))
            else:
                line = line.lower()
                words = re.split("\W+", line)
                example_list.append(" ".join(words))

    with open("./data/shijie.txt", "w", encoding='utf8') as f:
        [f.write(ex+"\n") for ex in example_list ]



def train(filename:str):
    model = fasttext.train_unsupervised(filename,  epoch = 10)
    model.save_model("./data/shijie.bin")
    wordlist = model.get_words()

    with open("./data/shijie.vec", "w", encoding='utf8') as f:
        firstline = "%d %d\n"%(len(wordlist), model.get_dimension())
        f.write(firstline)
        for w in wordlist:
            vec = model.get_word_vector(w)
            vecstr= ""
            for val in vec:
                vecstr += " "+str(val)
            f.write(w+" "+vecstr+"\n")
            #print(w)


def cosSim(v1:np.ndarray,v2:np.ndarray):
    return np.dot(v1,v2)/(np.linalg.norm(v1,ord=2)*np.linalg.norm(v2, ord=2))

def test_en(filename:str):
    model = fasttext.load_model(filename)
    print(model.get_word_id("father"))
    print(model.get_word_id("mother"))
    print(model.get_word_id("wife"))
    print(model.get_word_id("husband"))

    print(model.get_nearest_neighbors("father"))
    print(model.get_analogies("berlin", "germany", "france"))
    print(model.get_analogies("son", "father", "mother"))
    print(model.get_nearest_neighbors("females"))
    print(model.get_nearest_neighbors("see"))

def test_zh(filename:str):
    model = fasttext.load_model(filename)
    print(model.get_word_id("父亲"))
    print(model.get_word_id("母亲"))
    print(model.get_word_id("妻子"))
    print(model.get_word_id("丈夫"))

    print(model.get_nearest_neighbors("父亲"))
    print(model.get_analogies("柏林", "德国", "法国"))
    print(model.get_analogies("儿子", "父亲", "母亲"))

    print(model.get_nearest_neighbors("主要"))
    print(model.get_nearest_neighbors("米"))
    print(model.get_nearest_neighbors("由于"))

#loadData("E:\\DeepLearning\\data\\nlp_data\\平凡的世界.txt", True)
#loadData("E:\\DeepLearning\\data\\nlp_data\\English_corpus.txt", False)
#train("e:/DeepLearning/data/nlp_data/enwik9.txt")
#train("E:\\DeepLearning\\data\\nlp_data\\corpus\\wiki.zh.corpus.txt")
#test_zh("E:\\DeepLearning\\data\\fastText_model\\zhwiki_bymyself.bin")
test_en("E:\\DeepLearning\\data\\fastText_model\\enwiki9_bymyself.bin")
